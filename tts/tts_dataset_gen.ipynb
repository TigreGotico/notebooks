{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic TTS Training Dataset Generator\n",
    "\n",
    "Generates LJSpeech-style TTS training datasets from text prompts using a **single configurable donor TTS** voice and optional voice conversion.  \n",
    "When a folder of reference voices is provided, **one complete LJSpeech dataset is produced per reference voice**.\n",
    "\n",
    "You can use metadata.csv from an existing TTS dataset or find .txt prompts here:\n",
    "  - https://huggingface.co/datasets/fdemelo/phonetic-piper-recording-studio-prompts\n",
    "  - https://huggingface.co/collections/TigreGotico/synthetic-tts-datasets\n",
    "  - https://github.com/rhasspy/piper-recording-studio/tree/master/prompts\n",
    "\n",
    "### Pipeline stages\n",
    "\n",
    "| # | Stage | Description |\n",
    "|---|---|---|\n",
    "| 0 | Configuration | Set all paths, parameters, and flags via env vars |\n",
    "| 1 | Load Prompts | Read sentences from `prompts.txt`, a folder of `.txt` files, or `metadata.csv` |\n",
    "| 2 | Donor TTS Synthesis | Single configured TTS engine + voice generates all prompts once |\n",
    "| 3 | Post-Processing | Optional audio super-resolution, silence trimming, volume normalisation |\n",
    "| 4 | Voice Conversion + Dataset Assembly | For each reference voice: VC all utterances → build LJSpeech dataset (`filename|transcription`) |\n",
    "| 5 | Upload to HuggingFace | Optional push to HF (one repo per voice) |\n",
    "| 6 | Output Summary | File counts and sanity checks |\n",
    "\n",
    "### Output structure\n",
    "```\n",
    "base_dir/\n",
    "  1_donor_tts/           # Raw donor TTS audio (synthesised once)\n",
    "  2_processed/           # Post-processed donor audio\n",
    "  3_datasets/\n",
    "    voice_alice/         # ← One per reference voice\n",
    "      wavs/\n",
    "        LJ001-0001.wav\n",
    "      metadata.csv       # filename|transcription\n",
    "    donor_raw/           # ← If no VC\n",
    "      wavs/\n",
    "      metadata.csv\n",
    "```\n",
    "\n",
    "### Usage\n",
    "1. Edit the configuration cell below.\n",
    "2. Run cells sequentially.\n",
    "3. Each stage is self-contained — pip deps are installed per step.\n",
    "4. Existing output files are skipped on re-run (resume-safe).\n",
    "\n",
    "---\n",
    "\n",
    "> **Credits:** Based on pipelines from [TigreGotico/synthetic_dataset_generator](https://github.com/TigreGotico/synthetic_dataset_generator).  \n",
    "> Funded through the [NGI0 Commons Fund](https://nlnet.nl/commonsfund) via [NLnet](https://nlnet.nl), with support from the European Commission's Next Generation Internet programme (grant No 101135429)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0 · Global Configuration\n",
    "\n",
    "All parameters are set as environment variables via `os.environ.setdefault()`.  \n",
    "Override any of them from your shell, a `.env` file, or `%env` magic **before** running the config cell.\n",
    "\n",
    "**Edit the values below to match your setup, then run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# GENERAL\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"TTS_LANG\",            \"en\")                 # Language code (en, es, pt, …)\n",
    "os.environ.setdefault(\"TTS_BASE_DIR\",        \"/data/tts_dataset\")  # Root directory for all I/O\n",
    "os.environ.setdefault(\"TTS_DATASET_NAME\",    \"my_tts_dataset\")     # Base name for output datasets\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# INPUT SOURCES  (Stage 1)\n",
    "#   Set exactly ONE of these three:\n",
    "#   - PROMPTS_FILE   : plain text (one sentence per line) or\n",
    "#                      piper-recording-studio format (sentence_id<TAB>text)\n",
    "#   - PROMPTS_DIR    : folder of .txt files (each line = one sentence)\n",
    "#   - PROMPTS_CSV    : CSV/TSV with a 'text' or 'transcript' column\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"PROMPTS_FILE\",        \"prompts.txt\")        # Path to prompts.txt\n",
    "os.environ.setdefault(\"PROMPTS_DIR\",         \"\")                   # Path to folder of .txt files\n",
    "os.environ.setdefault(\"PROMPTS_CSV\",         \"\")                   # Path to metadata.csv\n",
    "os.environ.setdefault(\"CSV_TEXT_COL\",        \"text\")               # Column name for text in CSV\n",
    "os.environ.setdefault(\"CSV_SEP\",             \"|\")                  # CSV separator (| for LJSpeech style)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# DONOR TTS  (Stage 2)\n",
    "#   A single TTS engine + voice used to synthesise all prompts.\n",
    "#   Supported engines: edge, google, phoonnx\n",
    "#\n",
    "#   Edge voices   : `python -c \"from ovos_tts_plugin_edge_tts import VOICES; print(VOICES)\"`\n",
    "#                   examples: en-US-JennyNeural, en-US-GuyNeural, en-GB-SoniaNeural\n",
    "#   Google        : voice is ignored, only lang matters\n",
    "#   Phoonnx       : ONNX runtime TTS (supports all piper voices + more)\n",
    "#                   examples: OpenVoiceOS/phoonnx_pt-PT_miro_tugaphone\n",
    "#                   repo: https://github.com/TigreGotico/phoonnx\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"DONOR_ENGINE\",        \"edge\")               # edge | google | phoonnx\n",
    "os.environ.setdefault(\"DONOR_VOICE\",         \"en-US-JennyNeural\") # Voice identifier for chosen engine\n",
    "os.environ.setdefault(\"DONOR_RATE\",          \"+0%\")                # Speech rate (Edge only, e.g. +10%, -5%)\n",
    "os.environ.setdefault(\"DONOR_SLOW\",          \"0\")                  # Google only: 1 = slow speed\n",
    "\n",
    "# ── Phoonnx-specific options (only when DONOR_ENGINE=phoonnx) ────\n",
    "os.environ.setdefault(\"PHOONNX_PHONETIC_SPELL\", \"1\")               # Fix pronunciations for some words/langs\n",
    "os.environ.setdefault(\"PHOONNX_NOISE_SCALE\",    \"0.667\")           # Generator noise\n",
    "os.environ.setdefault(\"PHOONNX_LENGTH_SCALE\",   \"1.0\")             # Phoneme length (>1 = slower speech)\n",
    "os.environ.setdefault(\"PHOONNX_NOISE_W\",        \"0.8\")             # Phoneme width noise\n",
    "os.environ.setdefault(\"PHOONNX_DIACRITICS\",     \"0\")               # Add diacritics (Arabic/Hebrew only)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# VOICE CONVERSION  (Stage 4)\n",
    "#   A folder of reference voice WAVs.  One LJSpeech dataset is\n",
    "#   generated per reference voice file.\n",
    "#   Leave REF_VOICES_DIR empty to produce a single dataset from\n",
    "#   the raw donor TTS audio (no voice conversion).\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"REF_VOICES_DIR\",      \"\")                   # Folder of reference voice WAVs\n",
    "os.environ.setdefault(\"VC_DEVICE\",           \"cuda\")               # cuda | cpu\n",
    "os.environ.setdefault(\"VC_ENGINE\",           \"chatterbox\")         # chatterbox | chatterbox_onnx\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# POST-PROCESSING  (Stage 3)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"TARGET_SR\",           \"22050\")              # Final sample rate (22050 for LJSpeech)\n",
    "os.environ.setdefault(\"TARGET_DBFS\",         \"-20.0\")              # Volume normalisation target\n",
    "os.environ.setdefault(\"MIN_DURATION\",        \"0.5\")                # Seconds — discard shorter clips\n",
    "os.environ.setdefault(\"MAX_DURATION\",        \"15.0\")               # Seconds — discard longer clips\n",
    "os.environ.setdefault(\"TRIM_SILENCE\",        \"1\")                  # 1 = trim leading/trailing silence\n",
    "os.environ.setdefault(\"TRIM_TOP_DB\",         \"30\")                 # dB threshold for silence trimming\n",
    "\n",
    "# ── Audio Super-Resolution/Resampling ─\n",
    "#   Upscales ~16 kHz TTS audio to 48 kHz before final resample.\n",
    "#   Leave SR_ENGINE empty to use plain librosa resampling.\n",
    "#\n",
    "#   novasr  : 52 kB model, ~3500x RT on GPU. 16→48 kHz.\n",
    "#             https://github.com/ysharma3501/NovaSR\n",
    "#   flashsr : ~500 kB ONNX, ~200-400x RT. 16→48 kHz. CPU-friendly.\n",
    "#             https://github.com/ysharma3501/FlashSR\n",
    "#   lavasr  : Vocos-based speech enhancement + upsampling.\n",
    "#             https://github.com/ysharma3501/LavaSR\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"SR_ENGINE\",           \"\")                   # novasr | flashsr | lavasr | (empty=off)\n",
    "os.environ.setdefault(\"SR_DEVICE\",           \"cpu\")                # cuda | cpu  (novasr/lavasr only)\n",
    "os.environ.setdefault(\"LAVASR_DENOISE\",      \"0\")                  # 1 = enable LavaSR denoiser\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# HUGGINGFACE UPLOAD  (Stage 5)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"HF_UPLOAD\",           \"0\")                  # 1 = push to HuggingFace\n",
    "os.environ.setdefault(\"HF_REPO_PREFIX\",      \"\")                   # e.g. your-user/tts — voice name is appended\n",
    "os.environ.setdefault(\"HF_TOKEN\",            \"\")                   # HF write token (or use huggingface-cli login)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# Derived paths\n",
    "# ──────────────────────────────────────────────\n",
    "BASE = os.environ[\"TTS_BASE_DIR\"]\n",
    "LANG = os.environ[\"TTS_LANG\"]\n",
    "NAME = os.environ[\"TTS_DATASET_NAME\"]\n",
    "\n",
    "os.environ.setdefault(\"DIR_DONOR_TTS\",       os.path.join(BASE, \"1_donor_tts\"))\n",
    "os.environ.setdefault(\"DIR_PROCESSED\",       os.path.join(BASE, \"2_processed\"))\n",
    "os.environ.setdefault(\"DIR_DATASETS\",        os.path.join(BASE, \"3_datasets\"))\n",
    "\n",
    "\n",
    "def _env(key, default=\"\"):\n",
    "    return os.environ.get(key, default)\n",
    "\n",
    "\n",
    "def _flag(key):\n",
    "    return _env(key, \"0\").strip().lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  Language       : {LANG}\")\n",
    "print(f\"  Dataset name   : {NAME}\")\n",
    "print(f\"  Base directory : {BASE}\")\n",
    "print(f\"  Donor TTS      : {_env('DONOR_ENGINE')} / {_env('DONOR_VOICE')}\")\n",
    "if _env('DONOR_ENGINE') == 'phoonnx':\n",
    "    print(f\"    phonetic_spell={_flag('PHOONNX_PHONETIC_SPELL')}  noise_scale={_env('PHOONNX_NOISE_SCALE')}  \"\n",
    "          f\"length_scale={_env('PHOONNX_LENGTH_SCALE')}  noise_w={_env('PHOONNX_NOISE_W')}  \"\n",
    "          f\"diacritics={_flag('PHOONNX_DIACRITICS')}\")\n",
    "sr_engine = _env('SR_ENGINE')\n",
    "if sr_engine:\n",
    "    print(f\"  Super-res      : {sr_engine} on {_env('SR_DEVICE')}\")\n",
    "else:\n",
    "    print(f\"  Super-res      : off (librosa resample to {_env('TARGET_SR')} Hz)\")\n",
    "ref_dir = _env('REF_VOICES_DIR')\n",
    "if ref_dir:\n",
    "    from pathlib import Path as _P\n",
    "    n_refs = len(list(_P(ref_dir).rglob('*.wav'))) if _P(ref_dir).is_dir() else 0\n",
    "    print(f\"  VC enabled     : yes ({n_refs} reference voices in {ref_dir})\")\n",
    "    print(f\"  VC engine      : {_env('VC_ENGINE')} on {_env('VC_DEVICE')}\")\n",
    "else:\n",
    "    print(f\"  VC enabled     : no (single donor-only dataset)\")\n",
    "print(f\"  HF upload      : {'yes' if _flag('HF_UPLOAD') else 'no'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 · Load Prompts\n",
    "\n",
    "Reads text prompts from one of three sources (in priority order):\n",
    "1. `PROMPTS_CSV` — a CSV/TSV with a text column (pipe-separated LJSpeech style supported)\n",
    "2. `PROMPTS_DIR` — a folder of `.txt` files, each containing one sentence per line\n",
    "3. `PROMPTS_FILE` — a single text file with one sentence per line, or piper-recording-studio format (`sentence_id\\ttext`)\n",
    "\n",
    "Output is a deduplicated list of sentences stored in `PROMPTS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _clean_text(text):\n",
    "    \"\"\"Strip and collapse whitespace.  Returns empty string for junk.\"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text if len(text) > 1 else \"\"\n",
    "\n",
    "\n",
    "def load_prompts_from_file(path):\n",
    "    \"\"\"One sentence per line, or piper-recording-studio TSV: sentence_id<TAB>text.\"\"\"\n",
    "    lines = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # piper-recording-studio format: sentence_id<TAB>text\n",
    "            if \"\\t\" in line:\n",
    "                parts = line.split(\"\\t\", 1)\n",
    "                line = parts[-1]  # discard sentence_id, keep text\n",
    "            t = _clean_text(line)\n",
    "            if t:\n",
    "                lines.append(t)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_prompts_from_dir(folder):\n",
    "    \"\"\"Read every .txt file in folder, one sentence per line.\"\"\"\n",
    "    lines = []\n",
    "    for txt in sorted(Path(folder).rglob(\"*.txt\")):\n",
    "        lines.extend(load_prompts_from_file(txt))\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_prompts_from_csv(path, text_col, sep):\n",
    "    \"\"\"Read a CSV; auto-detects column by name or position.\"\"\"\n",
    "    import csv\n",
    "    lines = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Sniff whether there's a header\n",
    "        sample = f.read(4096)\n",
    "        f.seek(0)\n",
    "        has_header = csv.Sniffer().has_header(sample)\n",
    "        reader = csv.reader(f, delimiter=sep)\n",
    "\n",
    "        if has_header:\n",
    "            header = next(reader)\n",
    "            header_lower = [h.strip().lower() for h in header]\n",
    "            col_idx = None\n",
    "            for candidate in [text_col.lower(), \"text\", \"transcript\", \"transcription\",\n",
    "                              \"sentence\"]:\n",
    "                if candidate in header_lower:\n",
    "                    col_idx = header_lower.index(candidate)\n",
    "                    break\n",
    "            if col_idx is None:\n",
    "                # LJSpeech: filename|text — text is column 1 or last\n",
    "                col_idx = min(1, len(header) - 1)\n",
    "                print(f\"  Warning: text column '{text_col}' not found in header {header}.\")\n",
    "                print(f\"           Falling back to column index {col_idx}.\")\n",
    "        else:\n",
    "            # No header — assume LJSpeech: col 0 = filename, col 1 = text\n",
    "            col_idx = 1 if sep == \"|\" else 0\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) <= col_idx:\n",
    "                continue\n",
    "            t = _clean_text(row[col_idx])\n",
    "            if t:\n",
    "                lines.append(t)\n",
    "    return lines\n",
    "\n",
    "\n",
    "# ── Load from the configured source ──────────────────────────────────\n",
    "csv_path  = _env(\"PROMPTS_CSV\")\n",
    "dir_path  = _env(\"PROMPTS_DIR\")\n",
    "file_path = _env(\"PROMPTS_FILE\")\n",
    "\n",
    "raw_prompts = []\n",
    "if csv_path and os.path.exists(csv_path):\n",
    "    print(f\"Loading prompts from CSV: {csv_path}\")\n",
    "    raw_prompts = load_prompts_from_csv(csv_path, _env(\"CSV_TEXT_COL\", \"text\"), _env(\"CSV_SEP\", \"|\"))\n",
    "elif dir_path and os.path.isdir(dir_path):\n",
    "    print(f\"Loading prompts from directory: {dir_path}\")\n",
    "    raw_prompts = load_prompts_from_dir(dir_path)\n",
    "elif file_path and os.path.exists(file_path):\n",
    "    print(f\"Loading prompts from file: {file_path}\")\n",
    "    raw_prompts = load_prompts_from_file(file_path)\n",
    "else:\n",
    "    print(\"ERROR: No valid prompt source found.\")\n",
    "    print(\"  Set one of: PROMPTS_FILE, PROMPTS_DIR, or PROMPTS_CSV\")\n",
    "\n",
    "# Deduplicate while preserving order\n",
    "seen = set()\n",
    "PROMPTS = []\n",
    "for text in raw_prompts:\n",
    "    key = text.lower().strip()\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        PROMPTS.append(text)\n",
    "\n",
    "print(f\"  Loaded {len(raw_prompts)} lines -> {len(PROMPTS)} unique prompts\")\n",
    "if PROMPTS:\n",
    "    print(f\"  First: {PROMPTS[0][:80]}{'…' if len(PROMPTS[0]) > 80 else ''}\")\n",
    "    print(f\"  Last : {PROMPTS[-1][:80]}{'…' if len(PROMPTS[-1]) > 80 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Donor TTS Synthesis\n",
    "\n",
    "Uses the **single configured donor voice** to synthesise every prompt exactly once.  \n",
    "This is the \"master\" audio that gets voice-converted into each target voice in Stage 4.\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `DONOR_ENGINE` | `edge` / `google` / `phoonnx` |\n",
    "| `DONOR_VOICE` | Voice identifier (engine-specific) |\n",
    "| `DONOR_RATE` | Speech rate override (Edge only) |\n",
    "| `PHOONNX_*` | Phoonnx synthesis parameters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_engine = _env(\"DONOR_ENGINE\", \"edge\")\n",
    "if donor_engine == \"edge\":\n",
    "    !pip install --quiet --break-system-packages ovos_tts_plugin_edge_tts\n",
    "elif donor_engine == \"google\":\n",
    "    !pip install --quiet --break-system-packages ovos_tts_plugin_google_tx\n",
    "elif donor_engine == \"phoonnx\":\n",
    "    !pip install --quiet --break-system-packages phoonnx ovos-utils\n",
    "!pip install --quiet --break-system-packages tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Instantiate the single donor TTS plugin\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def _make_donor_plugin(engine: str, voice: str, lang: str, rate: str, slow: bool):\n",
    "    \"\"\"\n",
    "    Create a single TTS plugin instance from top-level config.\n",
    "    Returns (plugin, voice_id) where voice_id is used for filenames.\n",
    "    \"\"\"\n",
    "    if engine == \"edge\":\n",
    "        from ovos_tts_plugin_edge_tts import EdgeTTSPlugin\n",
    "        cfg = {\"voice\": voice}\n",
    "        if rate:\n",
    "            cfg[\"rate\"] = rate\n",
    "        return EdgeTTSPlugin(config=cfg), f\"edge_{voice}\"\n",
    "\n",
    "    if engine == \"google\":\n",
    "        from ovos_tts_plugin_google_tx import GoogleTranslateTTS\n",
    "        cfg = {\"lang\": lang, \"slow\": slow}\n",
    "        return GoogleTranslateTTS(config=cfg), f\"google_{lang}\"\n",
    "\n",
    "    if engine == \"phoonnx\":\n",
    "        from phoonnx.opm import PhoonnxTTSPlugin\n",
    "        cfg = {\n",
    "            \"lang\": lang,\n",
    "            \"enable_phonetic_spelling\": _flag(\"PHOONNX_PHONETIC_SPELL\"),\n",
    "            \"noise-scale\":  float(_env(\"PHOONNX_NOISE_SCALE\", \"0.667\")),\n",
    "            \"length-scale\": float(_env(\"PHOONNX_LENGTH_SCALE\", \"1.0\")),\n",
    "            \"noise-w\":      float(_env(\"PHOONNX_NOISE_W\", \"0.8\")),\n",
    "            \"add_diacritics\": _flag(\"PHOONNX_DIACRITICS\"),\n",
    "        }\n",
    "        if voice and voice != \"default\":\n",
    "            cfg[\"voice\"] = voice\n",
    "        return PhoonnxTTSPlugin(config=cfg), f\"phoonnx_{voice}\"\n",
    "\n",
    "    raise ValueError(f\"Unknown donor engine: {engine}\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Synthesis loop — one WAV per prompt, single donor voice\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def synthesize_with_donor(\n",
    "    prompts: list,\n",
    "    plugin,\n",
    "    voice_id: str,\n",
    "    lang: str,\n",
    "    output_dir: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Synthesise every prompt with the donor plugin.  Saves WAVs as\n",
    "    {idx:06d}.wav plus a JSONL manifest for traceability.\n",
    "    Resume-safe: skips prompts whose output WAV already exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    manifest_path = os.path.join(output_dir, \"tts_manifest.jsonl\")\n",
    "\n",
    "    # Load existing manifest entries for resume\n",
    "    existing = {}\n",
    "    if os.path.exists(manifest_path):\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    rec = json.loads(line)\n",
    "                    existing[rec[\"idx\"]] = rec\n",
    "        print(f\"  Resuming: {len(existing)} already synthesised.\")\n",
    "\n",
    "    manifest_f = open(manifest_path, \"a\", encoding=\"utf-8\")\n",
    "    success, fail, skipped = 0, 0, 0\n",
    "    result_map = dict(existing)\n",
    "    start = time.time()\n",
    "\n",
    "    for idx, text in enumerate(tqdm(prompts, desc=\"Donor TTS\", unit=\"utt\", file=sys.stdout)):\n",
    "        wav_name = f\"{idx:06d}.wav\"\n",
    "        wav_path = os.path.join(output_dir, wav_name)\n",
    "\n",
    "        if idx in existing and os.path.exists(wav_path):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            plugin.get_tts(text, wav_path, lang=lang)\n",
    "            rec = {\n",
    "                \"idx\": idx,\n",
    "                \"wav\": wav_name,\n",
    "                \"text\": text,\n",
    "                \"engine\": _env(\"DONOR_ENGINE\"),\n",
    "                \"voice\": voice_id,\n",
    "            }\n",
    "            manifest_f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            manifest_f.flush()\n",
    "            result_map[idx] = rec\n",
    "            success += 1\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  Failed #{idx}: {e}\")\n",
    "            fail += 1\n",
    "\n",
    "    manifest_f.close()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  Done: {success} new, {skipped} skipped, {fail} failed  ({elapsed:.1f}s)\")\n",
    "    return result_map\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Run donor TTS\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "if not PROMPTS:\n",
    "    print(\"No prompts loaded — run Stage 1 first.\")\n",
    "else:\n",
    "    engine   = _env(\"DONOR_ENGINE\", \"edge\")\n",
    "    voice    = _env(\"DONOR_VOICE\", \"en-US-JennyNeural\")\n",
    "    rate     = _env(\"DONOR_RATE\", \"+0%\")\n",
    "    slow     = _flag(\"DONOR_SLOW\")\n",
    "\n",
    "    print(f\"Donor: {engine} / {voice}\")\n",
    "    donor_plugin, donor_voice_id = _make_donor_plugin(engine, voice, LANG, rate, slow)\n",
    "\n",
    "    donor_dir = _env(\"DIR_DONOR_TTS\")\n",
    "    TTS_MANIFEST = synthesize_with_donor(\n",
    "        prompts=PROMPTS,\n",
    "        plugin=donor_plugin,\n",
    "        voice_id=donor_voice_id,\n",
    "        lang=LANG,\n",
    "        output_dir=donor_dir,\n",
    "    )\n",
    "    print(f\"\\nDonor TTS complete.  {len(TTS_MANIFEST)} utterances in {donor_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · Post-Processing\n",
    "\n",
    "For each donor WAV:\n",
    "1. **Audio super-resolution** (optional) — upscale ~16 kHz TTS to 48 kHz using NovaSR, FlashSR, or LavaSR\n",
    "2. Resample to target sample rate (default 22050 Hz for LJSpeech)\n",
    "3. Convert to mono\n",
    "4. Trim leading/trailing silence\n",
    "5. Normalise volume to target dBFS\n",
    "6. Filter by min/max duration\n",
    "\n",
    "When `SR_ENGINE` is set, super-resolution replaces the initial resample — the SR model\n",
    "upscales to 48 kHz, then a final `librosa.resample(48000 → TARGET_SR)` brings it to the\n",
    "desired output rate.\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `SR_ENGINE` | `novasr` / `flashsr` / `lavasr` / empty=off |\n",
    "| `SR_DEVICE` | `cuda` / `cpu` (novasr/lavasr) |\n",
    "| `LAVASR_DENOISE` | `1` = enable LavaSR denoiser |\n",
    "| `TARGET_SR` | Final sample rate (22050) |\n",
    "| `TARGET_DBFS` | Volume target in dBFS |\n",
    "| `MIN_DURATION` / `MAX_DURATION` | Duration filter (seconds) |\n",
    "| `TRIM_SILENCE` | 1 = trim silence |\n",
    "| `TRIM_TOP_DB` | Silence threshold (dB) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_engine = _env(\"SR_ENGINE\").strip().lower()\n",
    "if sr_engine == \"novasr\":\n",
    "    !pip install --quiet --break-system-packages git+https://github.com/ysharma3501/NovaSR.git\n",
    "elif sr_engine == \"flashsr\":\n",
    "    !pip install --quiet --break-system-packages git+https://github.com/ysharma3501/FlashSR.git\n",
    "elif sr_engine == \"lavasr\":\n",
    "    !pip install --quiet --break-system-packages git+https://github.com/ysharma3501/LavaSR.git\n",
    "!pip install --quiet --break-system-packages librosa soundfile numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Load SR model once (if enabled)\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def _load_sr_model(engine: str, device: str):\n",
    "    \"\"\"Load and return the super-resolution model + its output sample rate.\"\"\"\n",
    "    if engine == \"novasr\":\n",
    "        from NovaSR import FastSR\n",
    "        model = FastSR(device=device)\n",
    "        return model, 48000\n",
    "\n",
    "    if engine == \"flashsr\":\n",
    "        from FastAudioSR import FASR\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        ckpt = hf_hub_download(repo_id=\"YatharthS/FlashSR\", filename=\"upsampler.pth\", local_dir=\".\")\n",
    "        model = FASR(ckpt)\n",
    "        _ = model.model.half()\n",
    "        return model, 48000\n",
    "\n",
    "    if engine == \"lavasr\":\n",
    "        from LavaSR.model import LavaEnhance\n",
    "        model = LavaEnhance(\"YatharthS/LavaSR\", device)\n",
    "        return model, 16000\n",
    "\n",
    "    raise ValueError(f\"Unknown SR engine: {engine}\")\n",
    "\n",
    "\n",
    "def _apply_sr(wav_np: np.ndarray, sr_in: int, engine: str, model, sr_out: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Apply super-resolution to a numpy audio array.\n",
    "    Returns (upsampled_wav, new_sample_rate).\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    if engine == \"novasr\":\n",
    "        # NovaSR expects 16 kHz input loaded via its own loader, but also\n",
    "        # works with raw tensors.  Resample to 16k first if needed.\n",
    "        if sr_in != 16000:\n",
    "            wav_np = librosa.resample(wav_np, orig_sr=sr_in, target_sr=16000)\n",
    "        lowres = torch.from_numpy(wav_np).unsqueeze(0)\n",
    "        highres = model.infer(lowres).cpu().numpy().squeeze()\n",
    "        return highres, 48000\n",
    "\n",
    "    if engine == \"flashsr\":\n",
    "        if sr_in != 16000:\n",
    "            wav_np = librosa.resample(wav_np, orig_sr=sr_in, target_sr=16000)\n",
    "        lowres = torch.from_numpy(wav_np).unsqueeze(0).half()\n",
    "        highres = model.run(lowres).cpu().numpy().squeeze()\n",
    "        return highres, 48000\n",
    "\n",
    "    if engine == \"lavasr\":\n",
    "        if sr_in != 16000:\n",
    "            wav_np = librosa.resample(wav_np, orig_sr=sr_in, target_sr=16000)\n",
    "        lowres = torch.from_numpy(wav_np).unsqueeze(0)\n",
    "        denoise = _flag(\"LAVASR_DENOISE\")\n",
    "        highres = model.enhance(lowres, denoise=denoise).cpu().numpy().squeeze()\n",
    "        return highres, 16000  # LavaSR outputs at 16 kHz enhanced\n",
    "\n",
    "    # Should not reach here\n",
    "    return wav_np, sr_in\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Main post-processing loop\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def postprocess_audio(\n",
    "    manifest: dict,\n",
    "    source_dir: str,\n",
    "    output_dir: str,\n",
    "    target_sr: int = 22050,\n",
    "    target_dbfs: float = -20.0,\n",
    "    min_dur: float = 0.5,\n",
    "    max_dur: float = 15.0,\n",
    "    trim: bool = True,\n",
    "    trim_db: int = 30,\n",
    "    sr_engine: str = \"\",\n",
    "    sr_model=None,\n",
    "    sr_model_out_rate: int = 48000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Post-process each donor WAV: optional SR, resample, mono, trim, normalise, filter.\n",
    "    Returns a filtered manifest of successfully processed utterances.\n",
    "    Resume-safe: skips files that already exist in output_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    kept, dropped, skipped = 0, 0, 0\n",
    "    filtered_manifest = {}\n",
    "\n",
    "    for idx in tqdm(sorted(manifest.keys()), desc=\"Post-processing\", unit=\"utt\", file=sys.stdout):\n",
    "        rec = manifest[idx]\n",
    "        src_path = os.path.join(source_dir, rec[\"wav\"])\n",
    "        dst_path = os.path.join(output_dir, rec[\"wav\"])\n",
    "\n",
    "        # Resume check\n",
    "        if os.path.exists(dst_path):\n",
    "            try:\n",
    "                info = sf.info(dst_path)\n",
    "                dur = info.frames / info.samplerate\n",
    "                if min_dur <= dur <= max_dur:\n",
    "                    filtered_manifest[idx] = rec\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass  # re-process\n",
    "\n",
    "        if not os.path.exists(src_path):\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load at native SR first (for SR models that want 16k input)\n",
    "            wav, sr = librosa.load(src_path, sr=None, mono=True)\n",
    "\n",
    "            # Audio super-resolution (if enabled)\n",
    "            if sr_engine and sr_model is not None:\n",
    "                wav, sr = _apply_sr(wav, sr, sr_engine, sr_model, sr_model_out_rate)\n",
    "\n",
    "            # Resample to final target SR\n",
    "            if sr != target_sr:\n",
    "                wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)\n",
    "                sr = target_sr\n",
    "\n",
    "            if trim:\n",
    "                wav, _ = librosa.effects.trim(wav, top_db=trim_db)\n",
    "\n",
    "            dur = len(wav) / sr\n",
    "            if dur < min_dur or dur > max_dur:\n",
    "                dropped += 1\n",
    "                continue\n",
    "\n",
    "            # Volume normalisation\n",
    "            rms = np.sqrt(np.mean(wav ** 2) + 1e-9)\n",
    "            current_db = 20 * np.log10(rms + 1e-9)\n",
    "            gain_linear = 10 ** ((target_dbfs - current_db) / 20.0)\n",
    "            wav = np.clip(wav * gain_linear, -1.0, 1.0)\n",
    "\n",
    "            sf.write(dst_path, wav, sr)\n",
    "            filtered_manifest[idx] = rec\n",
    "            kept += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  Error processing {rec['wav']}: {e}\")\n",
    "            dropped += 1\n",
    "\n",
    "    print(f\"  Result: {kept} new + {skipped} existing = {kept + skipped} kept, {dropped} dropped\")\n",
    "    return filtered_manifest\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Run post-processing\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "sr_engine_name = _env(\"SR_ENGINE\").strip().lower()\n",
    "sr_model_obj, sr_out_rate = None, 48000\n",
    "\n",
    "if sr_engine_name:\n",
    "    print(f\"Loading SR model: {sr_engine_name} on {_env('SR_DEVICE', 'cpu')} ...\")\n",
    "    sr_model_obj, sr_out_rate = _load_sr_model(sr_engine_name, _env(\"SR_DEVICE\", \"cpu\"))\n",
    "    print(\"  SR model loaded.\")\n",
    "\n",
    "PROCESSED_MANIFEST = postprocess_audio(\n",
    "    manifest=TTS_MANIFEST,\n",
    "    source_dir=_env(\"DIR_DONOR_TTS\"),\n",
    "    output_dir=_env(\"DIR_PROCESSED\"),\n",
    "    target_sr=int(_env(\"TARGET_SR\", \"22050\")),\n",
    "    target_dbfs=float(_env(\"TARGET_DBFS\", \"-20.0\")),\n",
    "    min_dur=float(_env(\"MIN_DURATION\", \"0.5\")),\n",
    "    max_dur=float(_env(\"MAX_DURATION\", \"15.0\")),\n",
    "    trim=_flag(\"TRIM_SILENCE\"),\n",
    "    trim_db=int(_env(\"TRIM_TOP_DB\", \"30\")),\n",
    "    sr_engine=sr_engine_name,\n",
    "    sr_model=sr_model_obj,\n",
    "    sr_model_out_rate=sr_out_rate,\n",
    ")\n",
    "\n",
    "# Free SR model memory\n",
    "if sr_model_obj is not None:\n",
    "    del sr_model_obj\n",
    "    try:\n",
    "        import torch\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nPost-processing complete. {len(PROCESSED_MANIFEST)} utterances ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Voice Conversion + LJSpeech Dataset Assembly\n",
    "\n",
    "For **each reference voice WAV** in `REF_VOICES_DIR`:\n",
    "1. Voice-convert every processed donor utterance to that voice\n",
    "2. Assemble a complete LJSpeech dataset (`wavs/` + `metadata.csv`)\n",
    "\n",
    "If `REF_VOICES_DIR` is empty, a single dataset is built directly from the donor audio (no VC).\n",
    "\n",
    "### Output per voice\n",
    "```\n",
    "3_datasets/\n",
    "  {voice_stem}/\n",
    "    wavs/\n",
    "      LJ001-0001.wav\n",
    "      ...\n",
    "    metadata.csv   # filename|transcription  (no header)\n",
    "```\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `REF_VOICES_DIR` | Folder of reference voice WAVs |\n",
    "| `VC_ENGINE` | `chatterbox` or `chatterbox_onnx` |\n",
    "| `VC_DEVICE` | `cuda` / `cpu` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_voices_dir = _env(\"REF_VOICES_DIR\")\n",
    "vc_engine = _env(\"VC_ENGINE\", \"chatterbox\")\n",
    "\n",
    "if ref_voices_dir:\n",
    "    if vc_engine == \"chatterbox_onnx\":\n",
    "        !pip install --quiet --break-system-packages chatterbox_onnx tqdm\n",
    "    else:\n",
    "        !pip install --quiet --break-system-packages chatterbox torch torchaudio tqdm\n",
    "else:\n",
    "    print(\"REF_VOICES_DIR not set — will build a single donor-only dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def build_ljspeech_from_dir(\n",
    "    manifest: dict,\n",
    "    audio_dir: str,\n",
    "    output_dir: str,\n",
    "    chapter_prefix: str = \"LJ001\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Copy WAVs into wavs/ with LJSpeech naming and write metadata.csv.\n",
    "    Format: filename|transcription  (no header, pipe-separated).\n",
    "    Returns the number of utterances written.\n",
    "    \"\"\"\n",
    "    wavs_dir = os.path.join(output_dir, \"wavs\")\n",
    "    os.makedirs(wavs_dir, exist_ok=True)\n",
    "    meta_lines = []\n",
    "    count = 0\n",
    "\n",
    "    for idx in sorted(manifest.keys()):\n",
    "        rec = manifest[idx]\n",
    "        src = os.path.join(audio_dir, rec[\"wav\"])\n",
    "        if not os.path.exists(src):\n",
    "            continue\n",
    "        count += 1\n",
    "        lj_name = f\"{chapter_prefix}-{count:04d}\"\n",
    "        shutil.copy2(src, os.path.join(wavs_dir, f\"{lj_name}.wav\"))\n",
    "        meta_lines.append(f\"{lj_name}|{rec['text']}\")\n",
    "\n",
    "    meta_path = os.path.join(output_dir, \"metadata.csv\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(meta_lines) + \"\\n\")\n",
    "    return count\n",
    "\n",
    "\n",
    "def vc_and_build_dataset(\n",
    "    manifest: dict,\n",
    "    processed_dir: str,\n",
    "    ref_voice_path: str,\n",
    "    dataset_dir: str,\n",
    "    model,\n",
    "    engine: str,\n",
    "    target_sr: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Voice-convert every utterance to a single reference voice,\n",
    "    then assemble a LJSpeech dataset from the results.\n",
    "    Resume-safe: skips WAVs that already exist in the temp VC dir.\n",
    "    Cleans up intermediate VC files after assembly to save disk.\n",
    "    \"\"\"\n",
    "    vc_tmp = os.path.join(dataset_dir, \"_vc_tmp\")\n",
    "    os.makedirs(vc_tmp, exist_ok=True)\n",
    "\n",
    "    success, fail, skipped = 0, 0, 0\n",
    "\n",
    "    for idx in tqdm(\n",
    "        sorted(manifest.keys()),\n",
    "        desc=f\"  VC {Path(ref_voice_path).stem}\",\n",
    "        unit=\"utt\",\n",
    "        file=sys.stdout,\n",
    "    ):\n",
    "        rec = manifest[idx]\n",
    "        src_path = os.path.join(processed_dir, rec[\"wav\"])\n",
    "        dst_path = os.path.join(vc_tmp, rec[\"wav\"])\n",
    "\n",
    "        if os.path.exists(dst_path):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        if not os.path.exists(src_path):\n",
    "            fail += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if engine == \"chatterbox_onnx\":\n",
    "                model.voice_convert(\n",
    "                    source_audio_path=src_path,\n",
    "                    target_voice_path=ref_voice_path,\n",
    "                    output_file_name=dst_path,\n",
    "                )\n",
    "            else:\n",
    "                import torchaudio as ta\n",
    "                wav = model.generate(\n",
    "                    audio=src_path,\n",
    "                    target_voice_path=ref_voice_path,\n",
    "                )\n",
    "                ta.save(dst_path, wav, model.sr)\n",
    "            success += 1\n",
    "        except Exception:\n",
    "            fail += 1\n",
    "\n",
    "    print(f\"    VC: {success} new, {skipped} resumed, {fail} failed\")\n",
    "\n",
    "    # Assemble LJSpeech from the VC output\n",
    "    n = build_ljspeech_from_dir(manifest, vc_tmp, dataset_dir)\n",
    "    print(f\"    Dataset: {n} utterances -> {dataset_dir}\")\n",
    "\n",
    "    # Clean up temp VC files to save disk\n",
    "    shutil.rmtree(vc_tmp, ignore_errors=True)\n",
    "    return n\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Run: iterate over reference voices (or build donor-only dataset)\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "datasets_dir   = _env(\"DIR_DATASETS\")\n",
    "processed_dir  = _env(\"DIR_PROCESSED\")\n",
    "ref_voices_dir = _env(\"REF_VOICES_DIR\")\n",
    "vc_engine      = _env(\"VC_ENGINE\", \"chatterbox\")\n",
    "vc_device      = _env(\"VC_DEVICE\", \"cuda\")\n",
    "target_sr      = int(_env(\"TARGET_SR\", \"22050\"))\n",
    "\n",
    "BUILT_DATASETS = {}  # voice_name -> dataset_dir\n",
    "\n",
    "if not ref_voices_dir:\n",
    "    # ── No VC: build a single dataset from processed donor audio ──\n",
    "    print(\"No reference voices — building donor-only dataset.\")\n",
    "    ds_dir = os.path.join(datasets_dir, \"donor_raw\")\n",
    "    n = build_ljspeech_from_dir(PROCESSED_MANIFEST, processed_dir, ds_dir)\n",
    "    BUILT_DATASETS[\"donor_raw\"] = ds_dir\n",
    "    print(f\"  Built {n} utterances -> {ds_dir}\")\n",
    "\n",
    "else:\n",
    "    # ── Discover reference voices ─────────────────────────────────\n",
    "    ref_files = sorted(Path(ref_voices_dir).rglob(\"*.wav\"))\n",
    "    if not ref_files:\n",
    "        print(f\"ERROR: No .wav files found in {ref_voices_dir}\")\n",
    "    else:\n",
    "        print(f\"Found {len(ref_files)} reference voice(s). Loading VC model...\")\n",
    "\n",
    "        # Load model once, reuse for all voices\n",
    "        if vc_engine == \"chatterbox_onnx\":\n",
    "            from chatterbox_onnx import ChatterboxOnnx\n",
    "            vc_model = ChatterboxOnnx(device=vc_device)\n",
    "        else:\n",
    "            from chatterbox.vc import ChatterboxVC\n",
    "            vc_model = ChatterboxVC.from_pretrained(vc_device)\n",
    "        print(\"  VC model loaded.\\n\")\n",
    "\n",
    "        start_all = time.time()\n",
    "\n",
    "        for ri, ref_path in enumerate(ref_files):\n",
    "            voice_name = ref_path.stem  # e.g. \"alice\" from alice.wav\n",
    "            ds_dir = os.path.join(datasets_dir, voice_name)\n",
    "\n",
    "            # Skip if this dataset already looks complete\n",
    "            meta_file = os.path.join(ds_dir, \"metadata.csv\")\n",
    "            if os.path.exists(meta_file):\n",
    "                existing_n = sum(1 for line in open(meta_file) if line.strip())\n",
    "                if existing_n >= len(PROCESSED_MANIFEST):\n",
    "                    print(f\"[{ri+1}/{len(ref_files)}] {voice_name}: already complete ({existing_n} utts) — skipping.\")\n",
    "                    BUILT_DATASETS[voice_name] = ds_dir\n",
    "                    continue\n",
    "\n",
    "            print(f\"[{ri+1}/{len(ref_files)}] Processing voice: {voice_name}\")\n",
    "            n = vc_and_build_dataset(\n",
    "                manifest=PROCESSED_MANIFEST,\n",
    "                processed_dir=processed_dir,\n",
    "                ref_voice_path=str(ref_path),\n",
    "                dataset_dir=ds_dir,\n",
    "                model=vc_model,\n",
    "                engine=vc_engine,\n",
    "                target_sr=target_sr,\n",
    "            )\n",
    "            BUILT_DATASETS[voice_name] = ds_dir\n",
    "            print()\n",
    "\n",
    "        # Free GPU memory\n",
    "        del vc_model\n",
    "        try:\n",
    "            import torch\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        elapsed = time.time() - start_all\n",
    "        print(f\"\\nAll voices done. {len(BUILT_DATASETS)} datasets in {elapsed:.1f}s\")\n",
    "\n",
    "print(f\"\\nDatasets built: {list(BUILT_DATASETS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · Upload to HuggingFace (Optional)\n",
    "\n",
    "Pushes each per-voice LJSpeech dataset to HuggingFace.  \n",
    "Repo names are `{HF_REPO_PREFIX}_{voice_name}` (e.g. `user/tts_alice`).\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `HF_UPLOAD` | `1` to enable |\n",
    "| `HF_REPO_PREFIX` | e.g. `your-user/tts` — voice name is appended |\n",
    "| `HF_TOKEN` | Write token (optional if already logged in) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _flag(\"HF_UPLOAD\"):\n",
    "    !pip install --quiet --break-system-packages huggingface_hub\n",
    "else:\n",
    "    print(\"HF_UPLOAD not enabled — skipping install.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not _flag(\"HF_UPLOAD\"):\n",
    "    print(\"HF_UPLOAD not enabled — skipping upload.\")\n",
    "else:\n",
    "    repo_prefix = _env(\"HF_REPO_PREFIX\")\n",
    "    hf_token    = _env(\"HF_TOKEN\") or None\n",
    "\n",
    "    if not repo_prefix:\n",
    "        print(\"ERROR: HF_REPO_PREFIX not set.  Example: your-user/tts\")\n",
    "    else:\n",
    "        from huggingface_hub import HfApi\n",
    "        api = HfApi(token=hf_token)\n",
    "\n",
    "        for voice_name, ds_dir in BUILT_DATASETS.items():\n",
    "            repo_id = f\"{repo_prefix}_{voice_name}\"\n",
    "            print(f\"Uploading {voice_name} -> {repo_id} ...\")\n",
    "\n",
    "            api.create_repo(\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"dataset\",\n",
    "                exist_ok=True,\n",
    "            )\n",
    "            api.upload_folder(\n",
    "                folder_path=ds_dir,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"dataset\",\n",
    "                commit_message=f\"Upload TTS dataset: {_env('TTS_DATASET_NAME')} / {voice_name}\",\n",
    "            )\n",
    "            print(f\"  Done: https://huggingface.co/datasets/{repo_id}\")\n",
    "\n",
    "        print(f\"\\nAll {len(BUILT_DATASETS)} dataset(s) uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 · Output Summary\n",
    "\n",
    "Quick sanity check: list all output directories and count generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"  OUTPUT SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Pipeline dirs\n",
    "for label, d in [\n",
    "    (\"Donor TTS (raw)\",  _env(\"DIR_DONOR_TTS\")),\n",
    "    (\"Post-processed\",   _env(\"DIR_PROCESSED\")),\n",
    "]:\n",
    "    p = Path(d)\n",
    "    if p.exists():\n",
    "        wav_n = len(list(p.rglob(\"*.wav\")))\n",
    "        print(f\"  {label:25s}  {wav_n:>6} wav files\")\n",
    "    else:\n",
    "        print(f\"  {label:25s}  (not created)\")\n",
    "\n",
    "print(f\"{'─' * 65}\")\n",
    "print(f\"  {'DATASET':25s}  {'WAVs':>6}  {'meta lines':>10}\")\n",
    "print(f\"{'─' * 65}\")\n",
    "\n",
    "# Per-voice datasets\n",
    "datasets_root = Path(_env(\"DIR_DATASETS\"))\n",
    "if datasets_root.exists():\n",
    "    for ds_dir in sorted(datasets_root.iterdir()):\n",
    "        if not ds_dir.is_dir():\n",
    "            continue\n",
    "        wav_n = len(list((ds_dir / \"wavs\").rglob(\"*.wav\"))) if (ds_dir / \"wavs\").exists() else 0\n",
    "        meta = ds_dir / \"metadata.csv\"\n",
    "        meta_n = sum(1 for line in open(meta) if line.strip()) if meta.exists() else 0\n",
    "\n",
    "        marker = \"✓\" if wav_n > 0 and wav_n == meta_n else \"⚠\"\n",
    "        print(f\"  {marker} {ds_dir.name:23s}  {wav_n:>6}  {meta_n:>10}\")\n",
    "\n",
    "        # Preview first & last line of metadata\n",
    "        if meta.exists():\n",
    "            lines = meta.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "            if lines:\n",
    "                print(f\"    first: {lines[0][:90]}{'…' if len(lines[0]) > 90 else ''}\")\n",
    "            if len(lines) > 1:\n",
    "                print(f\"    last : {lines[-1][:90]}{'…' if len(lines[-1]) > 90 else ''}\")\n",
    "else:\n",
    "    print(\"  (no datasets created yet)\")\n",
    "\n",
    "print(\"=\" * 65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
