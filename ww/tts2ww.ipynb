{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wake Word Dataset Generation & Augmentation Pipeline\n",
    "\n",
    "A notebook for generating, augmenting, and benchmarking wake-word datasets.\n",
    "\n",
    "datasets generated with this method: https://huggingface.co/collections/TigreGotico/synthetic-wakeword-datasets\n",
    "\n",
    "for not-wake-word audio see https://huggingface.co/collections/TigreGotico/notwakeword-datasets\n",
    "\n",
    "for voice cloning donors you can get nearly infinite variety via https://mlcommons.org/datasets/multilingual-spoken-words/\n",
    "\n",
    "ongoing wake-word training experiments: https://github.com/OpenVoiceOS/ww-trainer\n",
    "\n",
    "original repo: https://github.com/TigreGotico/synthetic_dataset_generator\n",
    "\n",
    "### Pipeline stages\n",
    "\n",
    "| # | Stage | Description |\n",
    "|---|---|---|\n",
    "| 0 | Configuration | Set all paths, parameters, and flags via env vars |\n",
    "| 1 | Adversarial Word Generation | Phonetically confusable hard-negatives (LLM + grapheme edits) |\n",
    "| 2 | Normalize Word Lists | Lowercase, sort, deduplicate |\n",
    "| 3 | TTS Synthesis + Voice Conversion | Multi-engine TTS → optional Chatterbox VC |\n",
    "| 4 | Voice Cloning Augmentation | Revoice existing samples via `chatterbox_bulk_vc` |\n",
    "| 5 | VC TTS Synthesis | Direct Chatterbox bulk TTS with varied exaggeration |\n",
    "| 6 | Training Augmentation | Noise / reverb / pitch / speed for training data |\n",
    "| 7 | Benchmark Generation | Structured test sets at fixed SNRs for evaluation |\n",
    "\n",
    "---\n",
    "\n",
    "> **Credits:** Funded through the [NGI0 Commons Fund](https://nlnet.nl/commonsfund) via [NLnet](https://nlnet.nl), with support from the European Commission's Next Generation Internet programme (grant No 101135429)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0 · Global Configuration\n",
    "\n",
    "All parameters are set as environment variables via `os.environ.setdefault()`.  \n",
    "Override any of them from your shell, a `.env` file, or `%env` magic **before** running the config cell.\n",
    "\n",
    "**Edit the values below to match your setup, then run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# GENERAL\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"WW_LANG\",            \"en\")                 # Language code (en, es, pt, …)\n",
    "os.environ.setdefault(\"WW_WORDS\",           \"hey_mycroft\")        # Space-separated wake words (use _ for multi-word)\n",
    "os.environ.setdefault(\"WW_BASE_DIR\",        \"/data/ww\")           # Root directory for all I/O\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# ADVERSARIAL GENERATION  (Stage 1)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"ADV_N_SAMPLES\",      \"20\")                 # Target unique samples per wake word\n",
    "os.environ.setdefault(\"ADV_LLM_URL\",        \"http://localhost:11434\")  # Ollama / LLM API URL\n",
    "os.environ.setdefault(\"ADV_LLM_MODEL\",      \"gemma3:4b\")          # LLM model name\n",
    "os.environ.setdefault(\"ADV_LLM_WEIGHT\",     \"1.0\")                # Weight for LLM-based generation (0 = skip)\n",
    "os.environ.setdefault(\"ADV_GRAPH_WEIGHT\",   \"0.2\")                # Weight for GraphemeAug generation (0 = skip)\n",
    "os.environ.setdefault(\"ADV_GRAPH_MIN_EDIT\", \"2\")                  # Min Levenshtein distance\n",
    "os.environ.setdefault(\"ADV_GRAPH_MAX_EDIT\", \"3\")                  # Max Levenshtein distance\n",
    "os.environ.setdefault(\"ADV_FILE_MODE\",      \"append\")             # append | overwrite | error\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# TTS SYNTHESIS + VOICE CONVERSION  (Stage 3)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"SYNTH_N\",            \"900\")                # Samples per wake word\n",
    "os.environ.setdefault(\"SYNTH_USE_EDGE\",     \"1\")                  # 1 = enable Edge TTS\n",
    "os.environ.setdefault(\"SYNTH_USE_GOOGLE\",   \"1\")                  # 1 = enable Google TTS\n",
    "os.environ.setdefault(\"SYNTH_USE_PIPER\",    \"0\")                  # 1 = enable Piper TTS\n",
    "os.environ.setdefault(\"SYNTH_SEED\",         \"\")                   # Random seed (empty = none)\n",
    "os.environ.setdefault(\"SYNTH_VC_REFS\",      \"\")                   # Reference voice WAVs dir (empty = no VC)\n",
    "os.environ.setdefault(\"SYNTH_VC_DEVICE\",    \"cuda\")               # VC model device: cuda | cpu\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# VOICE CLONING AUGMENTATION  (Stage 4)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"VC_AUG_N_RANDOM\",    \"1\")                  # Random voice refs per sample\n",
    "os.environ.setdefault(\"VC_AUG_VOICES_PATH\", \"\")                   # Voice reference dataset dir\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# VC TTS SYNTHESIS  (Stage 5)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"VC_TTS_VOICES_PATH\",      \"\")              # Voice references dir\n",
    "os.environ.setdefault(\"VC_TTS_EXAGG_RANGE\",      \"0.4 0.9 0.1\")  # Exaggeration range: min max step\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# TRAINING AUGMENTATION  (Stage 6)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"AUG_INPUT_DIR\",      \"\")                   # Preprocessed dataset dir (with metadata.csv)\n",
    "os.environ.setdefault(\"AUG_BG_NOISE_DIR\",   \"\")                   # General background noise\n",
    "os.environ.setdefault(\"AUG_MUSIC_DIR\",      \"\")                   # Music folder\n",
    "os.environ.setdefault(\"AUG_BG_SPEECH_DIR\",  \"\")                   # Competing-speaker / babble\n",
    "os.environ.setdefault(\"AUG_MIC_NOISE_DIR\",  \"\")                   # Mic-specific silence / hiss\n",
    "os.environ.setdefault(\"AUG_RIR_DIR\",        \"\")                   # Room impulse responses\n",
    "os.environ.setdefault(\"AUG_SNR_MIN\",        \"0\")                  # dB\n",
    "os.environ.setdefault(\"AUG_SNR_MAX\",        \"20\")                 # dB\n",
    "os.environ.setdefault(\"AUG_PITCH_MIN\",      \"-1.0\")               # semitones\n",
    "os.environ.setdefault(\"AUG_PITCH_MAX\",      \"1.0\")\n",
    "os.environ.setdefault(\"AUG_SPEED_MIN\",      \"0.95\")\n",
    "os.environ.setdefault(\"AUG_SPEED_MAX\",      \"1.05\")\n",
    "os.environ.setdefault(\"AUG_PROB\",           \"0.9\")                # Per-sample augmentation probability\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# BENCHMARK GENERATION  (Stage 7)\n",
    "# ──────────────────────────────────────────────\n",
    "os.environ.setdefault(\"BENCH_CLEAN_DIR\",    \"\")                   # Clean wake-word samples\n",
    "os.environ.setdefault(\"BENCH_NOISE_DIR\",    \"\")                   # Ambient / environmental noise\n",
    "os.environ.setdefault(\"BENCH_MUSIC_DIR\",    \"\")                   # Music noise\n",
    "os.environ.setdefault(\"BENCH_SPEECH_DIR\",   \"\")                   # Competing-speaker audio\n",
    "os.environ.setdefault(\"BENCH_RIR_DIR\",      \"\")                   # RIR files\n",
    "os.environ.setdefault(\"BENCH_SNRS\",         \"20,15,10,5,0\")       # Comma-separated SNR list (dB)\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# Derived paths\n",
    "# ──────────────────────────────────────────────\n",
    "BASE    = os.environ[\"WW_BASE_DIR\"]\n",
    "LANG    = os.environ[\"WW_LANG\"]\n",
    "WW_LIST = os.environ[\"WW_WORDS\"].split()\n",
    "\n",
    "os.environ.setdefault(\"SYNTH_OUTPUT_DIR\",   os.path.join(BASE, \"synth_output\", LANG))\n",
    "os.environ.setdefault(\"VC_AUG_OUTPUT_DIR\",  os.path.join(BASE, \"vc_output\"))\n",
    "os.environ.setdefault(\"VC_TTS_OUTPUT_DIR\",  os.path.join(BASE, \"vc_synth_output\"))\n",
    "os.environ.setdefault(\"ADV_OUTPUT_DIR\",     os.path.join(BASE, \"adversarial\"))\n",
    "os.environ.setdefault(\"AUG_OUTPUT_DIR\",     os.path.join(BASE, \"augmented_data\"))\n",
    "os.environ.setdefault(\"BENCH_OUTPUT_DIR\",   os.path.join(BASE, \"benchmark_data\"))\n",
    "\n",
    "def _env(key, default=\"\"):\n",
    "    return os.environ.get(key, default)\n",
    "\n",
    "def _flag(key):\n",
    "    return _env(key, \"0\").strip().lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  Language       : {LANG}\")\n",
    "print(f\"  Wake words     : {WW_LIST}\")\n",
    "print(f\"  Base directory : {BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 · Adversarial Word Generation\n",
    "\n",
    "Generates phonetically confusable hard-negatives for each wake word using:\n",
    "- **GraphemeAug** - [arxiv](https://arxiv.org/abs/2505.14814v2) - single-grapheme edits (insertion / deletion / substitution) constrained by Levenshtein distance\n",
    "- **LLM** - prompted to produce rhyming, rhythm-matching adversarial phrases\n",
    "\n",
    "The two methods are weighted by `ADV_LLM_WEIGHT` and `ADV_GRAPH_WEIGHT`.\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `ADV_LLM_URL` / `ADV_LLM_MODEL` | LLM endpoint |\n",
    "| `ADV_N_SAMPLES` | Total unique target per word |\n",
    "| `ADV_LLM_WEIGHT` / `ADV_GRAPH_WEIGHT` | Relative generation weights |\n",
    "| `ADV_FILE_MODE` | `append` \\| `overwrite` \\| `error` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install --quiet --break-system-packages requests click"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Set, Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# GraphemeAugmenter — single-grapheme-edit hard-negative generation\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "class GraphemeAugmenter:\n",
    "    \"\"\"\n",
    "    Generates hard-negative (confusable) keyword-spotting examples based on\n",
    "    single-grapheme edits (Insertion, Deletion, Substitution), filtered by\n",
    "    Levenshtein distance from the original keyword.\n",
    "    \"\"\"\n",
    "    STANDARD_VOWELS = set('aeiou')\n",
    "    STANDARD_CONSONANTS = set('bcdfghjklmnpqrstvwxyz')\n",
    "\n",
    "    def __init__(self, max_edits: int = 1, min_edits: int = 1,\n",
    "                 language_vowels: Optional[Set[str]] = None,\n",
    "                 language_consonants: Optional[Set[str]] = None):\n",
    "        if max_edits < 1:\n",
    "            raise ValueError(\"max_edits must be 1 or greater.\")\n",
    "        if min_edits < 0:\n",
    "            raise ValueError(\"min_edits cannot be negative.\")\n",
    "        if min_edits > max_edits:\n",
    "            raise ValueError(\"min_edits cannot be greater than max_edits.\")\n",
    "\n",
    "        self.max_edits = max_edits\n",
    "        self.min_edits = min_edits\n",
    "        self.VOWELS = self.STANDARD_VOWELS.union({c.lower() for c in (language_vowels or set())})\n",
    "        self.CONSONANTS = self.STANDARD_CONSONANTS.union({c.lower() for c in (language_consonants or set())})\n",
    "        self.ALL_GRAPHEMES = self.VOWELS.union(self.CONSONANTS)\n",
    "\n",
    "    def _get_char_class(self, char: str) -> Optional[str]:\n",
    "        c = char.lower()\n",
    "        if c in self.VOWELS:\n",
    "            return 'vowel'\n",
    "        elif c in self.CONSONANTS:\n",
    "            return 'consonant'\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _levenshtein_distance(s1: str, s2: str) -> int:\n",
    "        if len(s1) < len(s2):\n",
    "            return GraphemeAugmenter._levenshtein_distance(s2, s1)\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        previous_row = list(range(len(s2) + 1))\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                cost = 0 if c1 == c2 else 1\n",
    "                current_row.append(min(\n",
    "                    previous_row[j + 1] + 1,\n",
    "                    current_row[j] + 1,\n",
    "                    previous_row[j] + cost\n",
    "                ))\n",
    "            previous_row = current_row\n",
    "        return previous_row[-1]\n",
    "\n",
    "    def _get_random_one_edit(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return random.choice(list(self.ALL_GRAPHEMES))\n",
    "        edit_type = random.randint(0, 2)\n",
    "\n",
    "        if edit_type == 0:  # Insertion\n",
    "            pos = random.randint(0, len(text))\n",
    "            g = random.choice(list(self.ALL_GRAPHEMES))\n",
    "            return text[:pos] + g + text[pos:]\n",
    "\n",
    "        elif edit_type == 1:  # Deletion\n",
    "            pos = random.randint(0, len(text) - 1)\n",
    "            return text[:pos] + text[pos + 1:]\n",
    "\n",
    "        else:  # Substitution (same-class: vowel↔vowel, consonant↔consonant)\n",
    "            for _ in range(10):\n",
    "                pos = random.randint(0, len(text) - 1)\n",
    "                char_class = self._get_char_class(text[pos])\n",
    "                if char_class == 'vowel':\n",
    "                    target_set = self.VOWELS\n",
    "                elif char_class == 'consonant':\n",
    "                    target_set = self.CONSONANTS\n",
    "                else:\n",
    "                    continue\n",
    "                possible = list(target_set - {text[pos].lower()})\n",
    "                if possible:\n",
    "                    return text[:pos] + random.choice(possible) + text[pos + 1:]\n",
    "            return self._get_random_one_edit(text)  # fallback\n",
    "\n",
    "    def generate_confusables(self, keyword: str, n_samples: int) -> List[str]:\n",
    "        original = keyword.lower()\n",
    "        generated: Set[str] = set()\n",
    "        if n_samples <= 0:\n",
    "            return []\n",
    "\n",
    "        seed_pool = {original}\n",
    "        attempts = 0\n",
    "        max_attempts = n_samples * 10 + 100\n",
    "\n",
    "        while len(generated) < n_samples and attempts < max_attempts:\n",
    "            word = random.choice(list(seed_pool))\n",
    "            candidate = self._get_random_one_edit(word)\n",
    "            distance = self._levenshtein_distance(original, candidate)\n",
    "            if (self.min_edits <= distance <= self.max_edits\n",
    "                    and candidate != original\n",
    "                    and candidate not in generated):\n",
    "                generated.add(candidate)\n",
    "                if distance < self.max_edits:\n",
    "                    seed_pool.add(candidate)\n",
    "            elif distance < self.max_edits and candidate != original:\n",
    "                seed_pool.add(candidate)\n",
    "            attempts += 1\n",
    "\n",
    "        return sorted(list(generated))\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# LLM-based adversarial generation (Ollama API)\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "_ADV_SYSTEM_PROMPT = \"\"\"You are an expert in computational linguistics, phonetics, and adversarial machine learning. Your core function is to generate lists of strings that are acoustically and linguistically similar to a given wake word or keyword. These generated strings serve as adversarial samples intended to trick an ASR system or keyword spotter.\n",
    "\n",
    "Generation Goal: The adversarial samples must primarily rhyme with the components of the input keyword or mimic its overall rhythm and length.\n",
    "\n",
    "Output Constraints (CRITICAL):\n",
    "1. Format: Output only the generated adversarial strings.\n",
    "2. Delimiter: Use a newline to separate each sample (one per line).\n",
    "3. Exclusion: DO NOT include any introductory text, numbering, bullet points, explanations, or concluding remarks.\n",
    "4. Diversity: All samples MUST be unique and PHONETICALLY similar.\n",
    "\n",
    "Example:\n",
    "[Input Keyword]: \"hey computer\"\n",
    "[Target Output]:\n",
    "say scooter\n",
    "pay intruder\n",
    "gray maneuver\n",
    "stay pewter\"\"\"\n",
    "\n",
    "_ADV_USER_TEMPLATE = \"Generate {n_samples} adversarial, rhyming samples for the following keyword: {word}\"\n",
    "\n",
    "\n",
    "def generate_llm_samples(url: str, model: str, wakeword: str, n_samples: int) -> List[str]:\n",
    "    \"\"\"Call an Ollama-compatible /api/generate endpoint for adversarial phrases.\"\"\"\n",
    "    if n_samples <= 0:\n",
    "        return []\n",
    "    print(f\"  LLM (target: {n_samples}) ... \", end=\"\", flush=True)\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            f\"{url}/api/generate\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": _ADV_USER_TEMPLATE.format(word=wakeword, n_samples=n_samples * 2),\n",
    "                \"system\": _ADV_SYSTEM_PROMPT,\n",
    "                \"stream\": False,\n",
    "            },\n",
    "            timeout=60,\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        result = resp.json()[\"response\"].strip().split(\"\\n\")\n",
    "        word_count = len(wakeword.split())\n",
    "        filtered = [r.strip() for r in result if len(r.strip().split()) == word_count]\n",
    "        uniq = list(set(filtered))[:n_samples]\n",
    "        print(f\"got {len(uniq)}\")\n",
    "        return uniq\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_grapheme_samples(augmenter: GraphemeAugmenter, wakeword: str, n_samples: int) -> List[str]:\n",
    "    \"\"\"Generate adversarial samples using single-grapheme edits.\"\"\"\n",
    "    if n_samples <= 0:\n",
    "        return []\n",
    "    print(f\"  GraphemeAug (target: {n_samples}) ... \", end=\"\", flush=True)\n",
    "    try:\n",
    "        samples = augmenter.generate_confusables(wakeword, n_samples=n_samples)\n",
    "        print(f\"got {len(samples)}\")\n",
    "        return samples\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"Adversarial generation functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Run adversarial generation for each wake word\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "n_samples     = int(_env(\"ADV_N_SAMPLES\", \"20\"))\n",
    "llm_weight    = float(_env(\"ADV_LLM_WEIGHT\", \"1.0\"))\n",
    "graph_weight  = float(_env(\"ADV_GRAPH_WEIGHT\", \"0.2\"))\n",
    "llm_url       = _env(\"ADV_LLM_URL\")\n",
    "llm_model     = _env(\"ADV_LLM_MODEL\")\n",
    "file_mode     = _env(\"ADV_FILE_MODE\", \"append\")\n",
    "adv_out_dir   = _env(\"ADV_OUTPUT_DIR\")\n",
    "\n",
    "total_weight = llm_weight + graph_weight\n",
    "if total_weight <= 0:\n",
    "    print(\"Both weights are 0 — skipping adversarial generation.\")\n",
    "else:\n",
    "    llm_ratio   = llm_weight / total_weight\n",
    "    graph_ratio = graph_weight / total_weight\n",
    "    llm_n       = int(n_samples * llm_ratio)\n",
    "    graph_n     = n_samples - llm_n  # remainder goes to grapheme\n",
    "\n",
    "    print(f\"Target: {n_samples} samples/word  (LLM: {llm_n}, Grapheme: {graph_n})\")\n",
    "\n",
    "    augmenter = None\n",
    "    if graph_weight > 0:\n",
    "        augmenter = GraphemeAugmenter(\n",
    "            max_edits=int(_env(\"ADV_GRAPH_MAX_EDIT\", \"3\")),\n",
    "            min_edits=int(_env(\"ADV_GRAPH_MIN_EDIT\", \"2\")),\n",
    "        )\n",
    "\n",
    "    os.makedirs(adv_out_dir, exist_ok=True)\n",
    "\n",
    "    for ww in WW_LIST:\n",
    "        output_file = os.path.join(adv_out_dir, f\"{ww}.txt\")\n",
    "        print(f\"\\n--- Processing '{ww}' -> {output_file} ---\")\n",
    "\n",
    "        # Determine file mode and load existing samples for dedup in append mode\n",
    "        existing: Set[str] = set()\n",
    "        if os.path.exists(output_file):\n",
    "            if file_mode == \"error\":\n",
    "                raise FileExistsError(\n",
    "                    f\"Output file exists: {output_file}. \"\n",
    "                    f\"Set ADV_FILE_MODE=append or overwrite.\"\n",
    "                )\n",
    "            if file_mode == \"append\":\n",
    "                with open(output_file, \"r\") as f:\n",
    "                    existing = {l.strip() for l in f if l.strip()}\n",
    "                print(f\"  Loaded {len(existing)} existing samples (append mode)\")\n",
    "            open_mode = \"a\" if file_mode == \"append\" else \"w\"\n",
    "        else:\n",
    "            open_mode = \"w\"\n",
    "\n",
    "        all_samples: Set[str] = existing.copy()\n",
    "\n",
    "        # LLM generation\n",
    "        if llm_n > 0 and llm_url:\n",
    "            for s in generate_llm_samples(llm_url, llm_model, ww, llm_n):\n",
    "                s = s.strip()\n",
    "                if s and s not in all_samples:\n",
    "                    all_samples.add(s)\n",
    "\n",
    "        # Grapheme generation\n",
    "        if graph_n > 0 and augmenter:\n",
    "            for s in generate_grapheme_samples(augmenter, ww, graph_n):\n",
    "                s = s.strip()\n",
    "                if s and s not in all_samples:\n",
    "                    all_samples.add(s)\n",
    "\n",
    "        # Write results\n",
    "        new_samples = sorted(all_samples - existing) if open_mode == \"a\" else sorted(all_samples)\n",
    "        if new_samples:\n",
    "            with open(output_file, open_mode) as f:\n",
    "                prefix = \"\\n\" if (open_mode == \"a\" and os.path.getsize(output_file) > 0) else \"\"\n",
    "                f.write(prefix + \"\\n\".join(new_samples))\n",
    "\n",
    "        print(f\"  Wrote {len(new_samples)} new samples (total unique: {len(all_samples)})\")\n",
    "\n",
    "    print(\"\\nAdversarial generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Normalize Adversarial Word Lists\n",
    "\n",
    "Lowercase, sort, and deduplicate each generated text file in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_out_dir = _env(\"ADV_OUTPUT_DIR\")\n",
    "\n",
    "for ww in WW_LIST:\n",
    "    txt_file = os.path.join(adv_out_dir, f\"{ww}.txt\")\n",
    "    if not os.path.isfile(txt_file):\n",
    "        print(f\"Skipping (not found): {txt_file}\")\n",
    "        continue\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        lines = {l.strip().lower() for l in f if l.strip()}\n",
    "    with open(txt_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(sorted(lines)))\n",
    "    print(f\"Normalized {txt_file}: {len(lines)} unique entries\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · TTS Synthesis + Voice Conversion\n",
    "\n",
    "Uses Edge TTS, Google TTS, and/or Piper to generate wake-word audio samples.  \n",
    "Optionally applies voice conversion with a Chatterbox ONNX model and reference voice WAVs.\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `SYNTH_N` | Samples per wake word |\n",
    "| `SYNTH_USE_EDGE` / `SYNTH_USE_GOOGLE` / `SYNTH_USE_PIPER` | Toggle engines |\n",
    "| `SYNTH_VC_REFS` | Reference voices dir (empty = skip VC) |\n",
    "| `SYNTH_VC_DEVICE` | `cuda` or `cpu` |\n",
    "| `SYNTH_SEED` | Reproducibility seed |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --break-system-packages \\\n",
    "    ovos_tts_plugin_edge_tts \\\n",
    "    ovos_tts_plugin_google_tx \\\n",
    "    ovos_tts_plugin_piper \\\n",
    "    chatterbox_onnx \\\n",
    "    torch torchaudio rich click tqdm ovos-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from os.path import join\n",
    "from typing import Dict\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Collect metadata for all available TTS voice configurations\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def _collect_tts_metadata(\n",
    "    lang: str,\n",
    "    edge: bool = True,\n",
    "    google: bool = True,\n",
    "    piper: bool = False,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scans enabled TTS plugins and returns a list of metadata dicts.\n",
    "    Each dict has keys: type, name, config.\n",
    "    No TTS plugin is instantiated yet — this is just metadata.\n",
    "    \"\"\"\n",
    "    lang_prefix = lang.split(\"-\")[0].lower()\n",
    "    metadata = []\n",
    "\n",
    "    # --- Edge TTS voices ---\n",
    "    if edge:\n",
    "        from ovos_tts_plugin_edge_tts import VOICES\n",
    "        rate_variations = (\n",
    "            [f\"+{r}%\" for r in range(1, 35)]\n",
    "            + [f\"-{r}%\" for r in range(1, 30)]\n",
    "        )\n",
    "        for locale, voices in VOICES.items():\n",
    "            if not locale.startswith(lang_prefix):\n",
    "                continue\n",
    "            for v in voices:\n",
    "                rate = random.choice(rate_variations)\n",
    "                metadata.append({\n",
    "                    \"type\": \"edge\",\n",
    "                    \"name\": f\"edge_{v}\",\n",
    "                    \"config\": {\"voice\": v, \"rate\": rate},\n",
    "                })\n",
    "\n",
    "    # --- Google Translate TTS ---\n",
    "    if google:\n",
    "        metadata.append({\n",
    "            \"type\": \"google\",\n",
    "            \"name\": f\"google_{lang}\",\n",
    "            \"config\": {\"lang\": lang, \"slow\": random.choice([True, False])},\n",
    "        })\n",
    "\n",
    "    # --- Piper TTS ---\n",
    "    if piper:\n",
    "        from ovos_tts_plugin_piper.voice_models import get_available_voices\n",
    "        from ovos_utils.lang import standardize_lang_tag\n",
    "        for voice, data in get_available_voices(update_voices=False).items():\n",
    "            l = standardize_lang_tag(data[\"language\"][\"code\"])\n",
    "            if not l.startswith(lang_prefix):\n",
    "                continue\n",
    "            n_speakers = len(data[\"speaker_id_map\"])\n",
    "            voices_list = (\n",
    "                [f\"{voice}#{i}\" for i in range(n_speakers)]\n",
    "                if n_speakers > 0\n",
    "                else [voice]\n",
    "            )\n",
    "            for v in voices_list:\n",
    "                metadata.append({\n",
    "                    \"type\": \"piper\",\n",
    "                    \"name\": f\"piper_{l}_{v}\",\n",
    "                    \"config\": {\"lang\": l, \"voice\": v},\n",
    "                })\n",
    "\n",
    "    print(f\"  Total TTS configurations: {len(metadata)}\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _instantiate_plugin(meta: Dict):\n",
    "    \"\"\"Create a TTS plugin instance from a metadata dict.\"\"\"\n",
    "    if meta[\"type\"] == \"edge\":\n",
    "        from ovos_tts_plugin_edge_tts import EdgeTTSPlugin\n",
    "        return EdgeTTSPlugin(config=meta[\"config\"])\n",
    "    if meta[\"type\"] == \"google\":\n",
    "        from ovos_tts_plugin_google_tx import GoogleTranslateTTS\n",
    "        return GoogleTranslateTTS(config=meta[\"config\"])\n",
    "    if meta[\"type\"] == \"piper\":\n",
    "        from ovos_tts_plugin_piper import PiperTTSPlugin\n",
    "        return PiperTTSPlugin(config=meta[\"config\"])\n",
    "    raise ValueError(f\"Unknown TTS type: {meta['type']}\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Core synthesis + voice conversion loop\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def synthesize_and_convert(\n",
    "    wake_word: str,\n",
    "    lang: str,\n",
    "    output_dir: str,\n",
    "    reference_voices_dir: str,\n",
    "    n: int,\n",
    "    device: str = \"cuda\",\n",
    "    edge: bool = True,\n",
    "    google: bool = True,\n",
    "    piper: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate N samples, each with a randomly-chosen (TTS plugin, reference voice)\n",
    "    combination.  Saves results to output_dir.\n",
    "    \"\"\"\n",
    "    all_meta = _collect_tts_metadata(lang, edge=edge, google=google, piper=piper)\n",
    "    if not all_meta:\n",
    "        print(\"  No TTS metadata found — aborting.\")\n",
    "        return\n",
    "\n",
    "    # --- Load VC model + reference voices (optional) ---\n",
    "    vc_model = None\n",
    "    ref_voices: List[str] = []\n",
    "\n",
    "    if reference_voices_dir:\n",
    "        base_path = Path(reference_voices_dir)\n",
    "        ref_voices = [str(p) for p in base_path.glob(\"**/*.wav\")]\n",
    "        if not ref_voices:\n",
    "            print(f\"  No reference voices (.wav) found in {reference_voices_dir}\")\n",
    "            return\n",
    "        print(f\"  Found {len(ref_voices)} reference voices. Loading VC model...\")\n",
    "        from chatterbox_onnx import ChatterboxOnnx\n",
    "        vc_model = ChatterboxOnnx(device=device)\n",
    "        print(\"  VC model loaded.\")\n",
    "    else:\n",
    "        print(\"  Voice conversion not enabled (SYNTH_VC_REFS empty).\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    success, fail = 0, 0\n",
    "    start = time.time()\n",
    "\n",
    "    for i in tqdm(range(n), total=n, desc=\"Synth + VC\", unit=\"sample\", file=sys.stdout):\n",
    "        plugin_meta = random.choice(all_meta)\n",
    "\n",
    "        # Instantiate TTS on the fly\n",
    "        try:\n",
    "            plugin = _instantiate_plugin(plugin_meta)\n",
    "        except Exception as e:\n",
    "            fail += 1\n",
    "            continue\n",
    "\n",
    "        base_name = str(uuid4())[10:]\n",
    "        tts_path = join(output_dir, f\"{base_name}_tts.wav\")\n",
    "        vc_path  = join(output_dir, f\"{base_name}.wav\")\n",
    "\n",
    "        try:\n",
    "            # 1) Generate TTS audio\n",
    "            plugin.get_tts(\n",
    "                wake_word.replace(\"_\", \" \").replace(\"-\", \" \"),\n",
    "                tts_path,\n",
    "                lang=lang,\n",
    "                voice=plugin_meta[\"config\"].get(\"voice\"),\n",
    "            )\n",
    "\n",
    "            # 2) Voice conversion (if model loaded)\n",
    "            if vc_model is not None:\n",
    "                ref_file = random.choice(ref_voices)\n",
    "                try:\n",
    "                    vc_model.voice_convert(\n",
    "                        source_audio_path=tts_path,\n",
    "                        target_voice_path=ref_file,\n",
    "                        output_file_name=vc_path,\n",
    "                    )\n",
    "                    success += 1\n",
    "                    os.remove(tts_path)  # clean up intermediate TTS file\n",
    "                except Exception as e:\n",
    "                    fail += 1\n",
    "            else:\n",
    "                success += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            fail += 1\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  Completed {success} ok / {fail} failed in {elapsed:.1f}s\")\n",
    "\n",
    "\n",
    "print(\"TTS + VC functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Run TTS synthesis (+ optional VC) for each wake word\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_edge   = _flag(\"SYNTH_USE_EDGE\")\n",
    "use_google = _flag(\"SYNTH_USE_GOOGLE\")\n",
    "use_piper  = _flag(\"SYNTH_USE_PIPER\")\n",
    "\n",
    "if not (use_edge or use_google or use_piper):\n",
    "    print(\n",
    "        \"No TTS engine enabled — set at least one of \"\n",
    "        \"SYNTH_USE_EDGE / SYNTH_USE_GOOGLE / SYNTH_USE_PIPER to 1.\"\n",
    "    )\n",
    "else:\n",
    "    seed_val = _env(\"SYNTH_SEED\")\n",
    "    if seed_val:\n",
    "        random.seed(int(seed_val))\n",
    "        print(f\"Using random seed: {seed_val}\")\n",
    "\n",
    "    synth_out = _env(\"SYNTH_OUTPUT_DIR\")\n",
    "    synth_n   = int(_env(\"SYNTH_N\", \"900\"))\n",
    "    vc_refs   = _env(\"SYNTH_VC_REFS\") or None\n",
    "    vc_device = _env(\"SYNTH_VC_DEVICE\", \"cuda\")\n",
    "\n",
    "    for ww in WW_LIST:\n",
    "        ww_out = os.path.join(synth_out, ww)\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Synthesising: {ww}  ->  {ww_out}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        synthesize_and_convert(\n",
    "            wake_word=ww,\n",
    "            lang=LANG,\n",
    "            output_dir=ww_out,\n",
    "            reference_voices_dir=vc_refs,\n",
    "            n=synth_n,\n",
    "            device=vc_device,\n",
    "            edge=use_edge,\n",
    "            google=use_google,\n",
    "            piper=use_piper,\n",
    "        )\n",
    "\n",
    "    print(\"\\nTTS Synthesis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Voice Cloning Augmentation\n",
    "\n",
    "Revoices an existing dataset (e.g. output of Stage 3) with random speaker references using `chatterbox_bulk_vc` (a CLI tool installed via the `chatterbox_onnx` package).\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `VC_AUG_VOICES_PATH` | Directory of reference voice WAVs |\n",
    "| `VC_AUG_N_RANDOM` | Random voices per sample |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Reinstall task-specific deps --\n",
    "!pip install --quiet --break-system-packages chatterbox_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "vc_aug_voices = _env(\"VC_AUG_VOICES_PATH\")\n",
    "vc_aug_out    = _env(\"VC_AUG_OUTPUT_DIR\")\n",
    "synth_out     = _env(\"SYNTH_OUTPUT_DIR\")\n",
    "n_random      = _env(\"VC_AUG_N_RANDOM\", \"1\")\n",
    "\n",
    "if not vc_aug_voices:\n",
    "    print(\"VC_AUG_VOICES_PATH not set — skipping voice cloning augmentation.\")\n",
    "else:\n",
    "    for ww in WW_LIST:\n",
    "        ww_in  = os.path.join(synth_out, ww)\n",
    "        ww_out = os.path.join(vc_aug_out, ww)\n",
    "        os.makedirs(ww_out, exist_ok=True)\n",
    "\n",
    "        cmd = [\n",
    "            \"chatterbox_bulk_vc\",\n",
    "            \"--output-path\", ww_out,\n",
    "            \"--audios-path\", ww_in,\n",
    "            \"--voices-path\", vc_aug_voices,\n",
    "            \"--n-random\", n_random,\n",
    "        ]\n",
    "        print(f\">>> {' '.join(cmd)}\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "    print(\"\\nVoice cloning augmentation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · VC TTS Synthesis\n",
    "\n",
    "Direct TTS via `chatterbox_bulk_tts` (CLI from the `chatterbox_onnx` package) with multiple exaggeration levels and voice references.\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `VC_TTS_VOICES_PATH` | Voice references directory |\n",
    "| `VC_TTS_EXAGG_RANGE` | Exaggeration range (`min max step`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install --quiet --break-system-packages chatterbox_onnx"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_tts_voices = _env(\"VC_TTS_VOICES_PATH\")\n",
    "vc_tts_out    = _env(\"VC_TTS_OUTPUT_DIR\")\n",
    "exagg_range   = _env(\"VC_TTS_EXAGG_RANGE\", \"0.4 0.9 0.1\").split()\n",
    "\n",
    "if not vc_tts_voices:\n",
    "    print(\"VC_TTS_VOICES_PATH not set — skipping VC TTS synthesis.\")\n",
    "else:\n",
    "    for ww in WW_LIST:\n",
    "        ww_phrase = ww.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "        ww_out = os.path.join(vc_tts_out, ww)\n",
    "        os.makedirs(ww_out, exist_ok=True)\n",
    "\n",
    "        cmd = [\n",
    "            \"chatterbox_bulk_tts\",\n",
    "            \"--output-path\", ww_out,\n",
    "            \"--voices-path\", vc_tts_voices,\n",
    "            \"--exaggeration-range\", *exagg_range,\n",
    "            ww_phrase,\n",
    "        ]\n",
    "        print(f\">>> {' '.join(cmd)}\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "    print(\"\\nVC TTS synthesis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 · Training Data Augmentation\n",
    "\n",
    "Applies stochastic augmentations to preprocessed training data:\n",
    "background noise mixing, music mixing, competing speech, reverb, pitch shifting, and speed perturbation.\n",
    "\n",
    "Reads a `metadata.csv` (`path,label`) from `AUG_INPUT_DIR` and writes augmented copies + new metadata to `AUG_OUTPUT_DIR`.\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `AUG_INPUT_DIR` | Preprocessed dataset with `metadata.csv` |\n",
    "| `AUG_BG_NOISE_DIR` / `AUG_MUSIC_DIR` / `AUG_BG_SPEECH_DIR` / `AUG_MIC_NOISE_DIR` | Noise sources |\n",
    "| `AUG_RIR_DIR` | Room impulse responses |\n",
    "| `AUG_SNR_MIN` / `AUG_SNR_MAX` | SNR range (dB) |\n",
    "| `AUG_PITCH_MIN/MAX`, `AUG_SPEED_MIN/MAX` | Transform ranges |\n",
    "| `AUG_PROB` | Per-sample augmentation probability |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install --quiet --break-system-packages librosa soundfile numpy click tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Audio utility functions  (shared by Stages 6 and 7)\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_audio_mono(path, sr=16000):\n",
    "    \"\"\"Load an audio file, convert to mono, and resample to target sr.\"\"\"\n",
    "    try:\n",
    "        wav, orig_sr = sf.read(str(path))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        return None\n",
    "    if wav.ndim > 1:\n",
    "        wav = np.mean(wav, axis=1)\n",
    "    if orig_sr != sr:\n",
    "        wav = librosa.resample(wav.astype(np.float32), orig_sr=orig_sr, target_sr=sr)\n",
    "    return wav.astype(np.float32)\n",
    "\n",
    "\n",
    "def save_wav(path, wav, sr=16000):\n",
    "    \"\"\"Save a numpy array as a WAV file.\"\"\"\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    sf.write(str(path), wav, sr)\n",
    "\n",
    "\n",
    "def mix_audio(clean, bg, snr_db):\n",
    "    \"\"\"\n",
    "    Mix a background track into clean audio at a given SNR (dB).\n",
    "    Background is tiled/cropped to match the length of the clean audio.\n",
    "    \"\"\"\n",
    "    clean_len = len(clean)\n",
    "    if len(bg) < clean_len:\n",
    "        bg = np.tile(bg, int(np.ceil(clean_len / len(bg))))\n",
    "    start = random.randint(0, len(bg) - clean_len)\n",
    "    bg = bg[start:start + clean_len]\n",
    "\n",
    "    rms_clean = np.sqrt(np.mean(clean ** 2) + 1e-9)\n",
    "    rms_bg    = np.sqrt(np.mean(bg ** 2) + 1e-9)\n",
    "    desired   = rms_clean / (10 ** (snr_db / 20.0))\n",
    "    if rms_bg > 0:\n",
    "        bg = bg * (desired / rms_bg)\n",
    "\n",
    "    mixed = clean + bg\n",
    "    peak = np.max(np.abs(mixed))\n",
    "    if peak > 1.0:\n",
    "        mixed = mixed / peak\n",
    "    return mixed.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_reverb(wav, rir, attenuation=0.5):\n",
    "    \"\"\"Convolve with a Room Impulse Response, preserving original RMS (attenuated).\"\"\"\n",
    "    out = np.convolve(wav, rir)[:len(wav)]\n",
    "    rms_wav = np.sqrt(np.mean(wav ** 2) + 1e-9)\n",
    "    rms_out = np.sqrt(np.mean(out ** 2) + 1e-9)\n",
    "    out = out * (rms_wav / rms_out) * attenuation\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "def pitch_shift(wav, sr, n_steps):\n",
    "    \"\"\"Pitch-shift audio by n_steps semitones.\"\"\"\n",
    "    return librosa.effects.pitch_shift(wav, sr=sr, n_steps=n_steps).astype(np.float32)\n",
    "\n",
    "\n",
    "def speed_perturb(wav, factor):\n",
    "    \"\"\"Time-stretch audio by the given speed factor.\"\"\"\n",
    "    return librosa.effects.time_stretch(wav, rate=factor).astype(np.float32)\n",
    "\n",
    "\n",
    "def collect_audio_files(base_folder):\n",
    "    \"\"\"Recursively collect audio files (.wav .flac .mp3 .m4a .ogg).\"\"\"\n",
    "    exts = [\".wav\", \".flac\", \".mp3\", \".m4a\", \".ogg\"]\n",
    "    if not base_folder:\n",
    "        return []\n",
    "    p = Path(base_folder)\n",
    "    if not p.exists():\n",
    "        print(f\"Warning: folder not found: {base_folder}\")\n",
    "        return []\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(p.rglob(f\"*{ext}\"))\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "print(\"Audio utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Run training augmentation\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "aug_input_dir = _env(\"AUG_INPUT_DIR\")\n",
    "aug_out_dir   = _env(\"AUG_OUTPUT_DIR\")\n",
    "\n",
    "if not aug_input_dir:\n",
    "    print(\"AUG_INPUT_DIR not set — skipping training augmentation.\")\n",
    "else:\n",
    "    input_dir = Path(aug_input_dir)\n",
    "    out_dir   = Path(aug_out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metadata_in  = input_dir / \"metadata.csv\"\n",
    "    metadata_out = out_dir / \"metadata.csv\"\n",
    "\n",
    "    if not metadata_in.exists():\n",
    "        raise FileNotFoundError(f\"metadata.csv not found in {input_dir}\")\n",
    "\n",
    "    # Collect augmentation source files\n",
    "    bg_files      = collect_audio_files(_env(\"AUG_BG_NOISE_DIR\"))\n",
    "    music_files   = collect_audio_files(_env(\"AUG_MUSIC_DIR\"))\n",
    "    speech_files  = collect_audio_files(_env(\"AUG_BG_SPEECH_DIR\"))\n",
    "    rir_files     = collect_audio_files(_env(\"AUG_RIR_DIR\"))\n",
    "    mic_files     = collect_audio_files(_env(\"AUG_MIC_NOISE_DIR\"))\n",
    "\n",
    "    snr_min   = float(_env(\"AUG_SNR_MIN\", \"0\"))\n",
    "    snr_max   = float(_env(\"AUG_SNR_MAX\", \"20\"))\n",
    "    pitch_min = float(_env(\"AUG_PITCH_MIN\", \"-1.0\"))\n",
    "    pitch_max = float(_env(\"AUG_PITCH_MAX\", \"1.0\"))\n",
    "    speed_min = float(_env(\"AUG_SPEED_MIN\", \"0.95\"))\n",
    "    speed_max = float(_env(\"AUG_SPEED_MAX\", \"1.05\"))\n",
    "    prob      = float(_env(\"AUG_PROB\", \"0.9\"))\n",
    "\n",
    "    with open(metadata_in, \"r\", encoding=\"utf-8\") as f:\n",
    "        source_data = [line.strip().split(\",\") for line in f]\n",
    "\n",
    "    print(f\"Processing {len(source_data)} samples from {metadata_in}\")\n",
    "    print(f\"Augmentation probability: {prob*100:.0f}%\")\n",
    "\n",
    "    avail = []\n",
    "    if bg_files:     avail.append(f\"bg_noise ({len(bg_files)})\")\n",
    "    if music_files:  avail.append(f\"music ({len(music_files)})\")\n",
    "    if speech_files: avail.append(f\"speech ({len(speech_files)})\")\n",
    "    if rir_files:    avail.append(f\"rir ({len(rir_files)})\")\n",
    "    if mic_files:    avail.append(f\"mic ({len(mic_files)})\")\n",
    "    avail += [\"pitch_shift\", \"speed_perturb\"]\n",
    "    print(f\"Available augmentations: {', '.join(avail)}\")\n",
    "\n",
    "    augmented = []\n",
    "    aug_count = 0\n",
    "\n",
    "    for path_str, label in tqdm(source_data, desc=\"Augmenting\", unit=\"sample\"):\n",
    "        src_path = Path(path_str)\n",
    "        if not src_path.is_absolute():\n",
    "            src_path = input_dir.parent / src_path\n",
    "\n",
    "        try:\n",
    "            wav = load_audio_mono(src_path)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  Could not load {src_path}: {e}\")\n",
    "            continue\n",
    "        if wav is None:\n",
    "            continue\n",
    "\n",
    "        was_augmented = False\n",
    "\n",
    "        if random.random() < prob:\n",
    "            # 1. Background noise (50% chance)\n",
    "            if bg_files and random.random() < 0.5:\n",
    "                bg = load_audio_mono(random.choice(bg_files))\n",
    "                if bg is not None:\n",
    "                    wav = mix_audio(wav, bg, random.uniform(snr_min, snr_max))\n",
    "                    was_augmented = True\n",
    "\n",
    "            # Mic-specific noise (80% chance)\n",
    "            if mic_files and random.random() < 0.8:\n",
    "                mic = load_audio_mono(random.choice(mic_files))\n",
    "                if mic is not None:\n",
    "                    wav = mix_audio(wav, mic, random.uniform(snr_min, snr_max))\n",
    "                    was_augmented = True\n",
    "\n",
    "            # 2. Music mixing (50% chance, louder SNR 0-10 dB)\n",
    "            if music_files and random.random() < 0.5:\n",
    "                music = load_audio_mono(random.choice(music_files))\n",
    "                if music is not None:\n",
    "                    wav = mix_audio(wav, music, random.uniform(0.0, 10.0))\n",
    "                    was_augmented = True\n",
    "\n",
    "            # 3. Background speech (60% chance, quieter SNR 10-25 dB)\n",
    "            if speech_files and random.random() < 0.6:\n",
    "                speech = load_audio_mono(random.choice(speech_files))\n",
    "                if speech is not None:\n",
    "                    wav = mix_audio(wav, speech, random.uniform(10.0, 25.0))\n",
    "                    was_augmented = True\n",
    "\n",
    "            # 4. Reverb (30% chance)\n",
    "            if rir_files and random.random() < 0.3:\n",
    "                rir = load_audio_mono(random.choice(rir_files))\n",
    "                if rir is not None:\n",
    "                    wav = apply_reverb(wav, rir)\n",
    "                    was_augmented = True\n",
    "\n",
    "            # 5. Pitch shift (30% chance)\n",
    "            if random.random() < 0.3:\n",
    "                wav = pitch_shift(wav, 16000, random.uniform(pitch_min, pitch_max))\n",
    "                was_augmented = True\n",
    "\n",
    "            # 6. Speed perturbation (30% chance)\n",
    "            if random.random() < 0.3:\n",
    "                wav = speed_perturb(wav, random.uniform(speed_min, speed_max))\n",
    "                was_augmented = True\n",
    "\n",
    "        if was_augmented:\n",
    "            aug_count += 1\n",
    "\n",
    "        # Peak-normalize and save\n",
    "        wav = wav / (np.max(np.abs(wav)) + 1e-9)\n",
    "        rel_path = Path(\"wakes_aug\" if int(label) == 1 else \"negatives_aug\") / (src_path.stem + \"_aug.wav\")\n",
    "        dst_path = out_dir / rel_path\n",
    "        save_wav(dst_path, wav)\n",
    "        augmented.append((str(dst_path), label))\n",
    "\n",
    "    with metadata_out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for p, lbl in augmented:\n",
    "            f.write(f\"{p},{lbl}\\n\")\n",
    "\n",
    "    print(f\"\\n--- Augmentation Complete ---\")\n",
    "    print(f\"  Generated {len(augmented)} samples ({aug_count} received transformations)\")\n",
    "    print(f\"  Metadata: {metadata_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 · Benchmark Dataset Generation\n",
    "\n",
    "Creates structured test sets at fixed SNRs and with acoustic variance (pitch shifts, speed perturbations, reverb) for evaluating false-negative rates.\n",
    "\n",
    "| Key env vars | |\n",
    "|---|---|\n",
    "| `BENCH_CLEAN_DIR` | Clean wake-word samples |\n",
    "| `BENCH_NOISE_DIR` / `BENCH_MUSIC_DIR` / `BENCH_SPEECH_DIR` | Noise sources |\n",
    "| `BENCH_RIR_DIR` | RIR files |\n",
    "| `BENCH_SNRS` | Comma-separated SNR list (e.g. `20,15,10,5,0`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install --quiet --break-system-packages librosa soundfile numpy click tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Benchmark augmentation: deterministic test sets\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def _bench_mix(clean_files, bg_files, snr_list, output_dir, noise_type):\n",
    "    \"\"\"\n",
    "    For each clean sample, mix with a random background file\n",
    "    at every SNR in snr_list.  Creates one output per (sample, SNR).\n",
    "    \"\"\"\n",
    "    if not bg_files:\n",
    "        print(f\"  Skipping {noise_type}: no background files.\")\n",
    "        return\n",
    "    if not snr_list:\n",
    "        return\n",
    "\n",
    "    print(f\"  Mixing {noise_type} at SNRs: {snr_list} dB\")\n",
    "    for clean_path in tqdm(clean_files, desc=f\"  {noise_type}\"):\n",
    "        clean_wav = load_audio_mono(clean_path)\n",
    "        if clean_wav is None:\n",
    "            continue\n",
    "        bg_path = random.choice(bg_files)\n",
    "        bg_wav = load_audio_mono(bg_path)\n",
    "        if bg_wav is None:\n",
    "            continue\n",
    "\n",
    "        stem = clean_path.stem\n",
    "        for snr in snr_list:\n",
    "            mixed = mix_audio(clean_wav, bg_wav, snr)\n",
    "            out_name = f\"{stem}_{noise_type}_{int(snr)}db.wav\"\n",
    "            save_wav(output_dir / noise_type.lower() / out_name, mixed)\n",
    "\n",
    "\n",
    "def _bench_variance(clean_files, rir_files, output_dir):\n",
    "    \"\"\"\n",
    "    Apply deterministic pitch, speed, and reverb perturbations to clean samples.\n",
    "    \"\"\"\n",
    "    PITCH_STEPS   = [-3, -2, -1, 1, 2, 3]  # semitones\n",
    "    SPEED_FACTORS = [0.9, 1.1]\n",
    "\n",
    "    print(\"  Applying acoustic variance (pitch / speed / reverb)\")\n",
    "    for clean_path in tqdm(clean_files, desc=\"  Variance\"):\n",
    "        clean_wav = load_audio_mono(clean_path)\n",
    "        if clean_wav is None:\n",
    "            continue\n",
    "        stem = clean_path.stem\n",
    "\n",
    "        # Pitch shifts\n",
    "        for step in PITCH_STEPS:\n",
    "            out = pitch_shift(clean_wav, 16000, step)\n",
    "            save_wav(output_dir / \"variance\" / f\"{stem}_pitch_{step}.wav\", out)\n",
    "\n",
    "        # Speed perturbations\n",
    "        for factor in SPEED_FACTORS:\n",
    "            out = speed_perturb(clean_wav, factor)\n",
    "            save_wav(output_dir / \"variance\" / f\"{stem}_speed_{factor:.1f}.wav\", out)\n",
    "\n",
    "        # Reverb (up to 3 randomly-selected RIRs per sample)\n",
    "        if rir_files:\n",
    "            selected = random.sample(rir_files, k=min(3, len(rir_files)))\n",
    "            for rir_path in selected:\n",
    "                rir_wav = load_audio_mono(rir_path)\n",
    "                if rir_wav is None:\n",
    "                    continue\n",
    "                out = apply_reverb(clean_wav, rir_wav)\n",
    "                rir_id = rir_path.stem[:8]\n",
    "                save_wav(output_dir / \"variance\" / f\"{stem}_reverb_{rir_id}.wav\", out)\n",
    "\n",
    "\n",
    "print(\"Benchmark functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Run benchmark generation\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "bench_clean_dir = _env(\"BENCH_CLEAN_DIR\")\n",
    "bench_out_dir   = Path(_env(\"BENCH_OUTPUT_DIR\"))\n",
    "\n",
    "if not bench_clean_dir:\n",
    "    print(\"BENCH_CLEAN_DIR not set — skipping benchmark generation.\")\n",
    "else:\n",
    "    clean_files  = collect_audio_files(bench_clean_dir)\n",
    "    noise_files  = collect_audio_files(_env(\"BENCH_NOISE_DIR\"))\n",
    "    music_files  = collect_audio_files(_env(\"BENCH_MUSIC_DIR\"))\n",
    "    speech_files = collect_audio_files(_env(\"BENCH_SPEECH_DIR\"))\n",
    "    rir_files    = collect_audio_files(_env(\"BENCH_RIR_DIR\"))\n",
    "\n",
    "    snr_str  = _env(\"BENCH_SNRS\", \"20,15,10,5,0\")\n",
    "    snr_list = [float(s.strip()) for s in snr_str.split(\",\") if s.strip()]\n",
    "\n",
    "    if not clean_files:\n",
    "        print(\"No clean files found — aborting.\")\n",
    "    else:\n",
    "        bench_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Found {len(clean_files)} clean files. Output -> {bench_out_dir}\")\n",
    "\n",
    "        _bench_mix(clean_files, noise_files,  snr_list, bench_out_dir, \"Noise\")\n",
    "        _bench_mix(clean_files, music_files,  snr_list, bench_out_dir, \"Music\")\n",
    "        _bench_mix(clean_files, speech_files, snr_list, bench_out_dir, \"Speech\")\n",
    "        _bench_variance(clean_files, rir_files, bench_out_dir)\n",
    "\n",
    "        print(f\"\\nBenchmark generation complete. Files saved under {bench_out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 · Output Summary\n",
    "\n",
    "Quick sanity check: list all output directories and count generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_to_check = {\n",
    "    \"Adversarial word lists\": _env(\"ADV_OUTPUT_DIR\"),\n",
    "    \"TTS Synth output\":      _env(\"SYNTH_OUTPUT_DIR\"),\n",
    "    \"VC augmented output\":   _env(\"VC_AUG_OUTPUT_DIR\"),\n",
    "    \"VC TTS output\":         _env(\"VC_TTS_OUTPUT_DIR\"),\n",
    "    \"Training augmented\":    _env(\"AUG_OUTPUT_DIR\"),\n",
    "    \"Benchmark test sets\":   _env(\"BENCH_OUTPUT_DIR\"),\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  OUTPUT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label, d in dirs_to_check.items():\n",
    "    p = Path(d)\n",
    "    if p.exists():\n",
    "        wav_n = len(list(p.rglob(\"*.wav\")))\n",
    "        txt_n = len(list(p.rglob(\"*.txt\")))\n",
    "        csv_n = len(list(p.rglob(\"*.csv\")))\n",
    "        parts = []\n",
    "        if wav_n: parts.append(f\"{wav_n} wav\")\n",
    "        if txt_n: parts.append(f\"{txt_n} txt\")\n",
    "        if csv_n: parts.append(f\"{csv_n} csv\")\n",
    "        total = wav_n + txt_n + csv_n\n",
    "        print(f\"  {label:30s}  {total:>6} files  ({', '.join(parts) if parts else 'empty'})\")\n",
    "    else:\n",
    "        print(f\"  {label:30s}  (not created)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
